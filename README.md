# voice-to-text-agent

ระบบแปลงเสียงพูดหรือ transcript เป็น Structured Data โดยออกแบบมาเพื่อรองรับเสียงพูดที่ไม่ครบหรือไม่ชัด โดยมีการใช้ Speech-to-Text (STT), Large Language Model (LLM) และกฎแบบ Rule-based เพื่อให้ได้ผลลัพธ์ที่ถูกต้องและเชื่อถือได้

--------
## ภาพรวมของระบบ (Overview)

ระบบนี้ถูกออกแบบมาเพื่อดึงข้อมูลต่อไปนี้จากเสียงพูดของผู้ใช้ รองรับทั้งการพูดภาษาไทยและอังกฤษ

- `first_name` : ชื่อ
- `last_name` : นามสกุล
- `gender` : เพศ
- `phone` : เบอร์โทรศัพท์
- `license_plate` : ป้ายทะเบียนรถ

ระบบมีการทำงาน 2 แบบ คือ
1. การประมวลผลครั้งเดียว (Single-pass pipeline) : ใส่เสียงเข้ามา หากข้อมูลครบและถูกต้อง จะขึ้นแจ้งเตือนว่า complete หากข้อมูลไม่ครบหรือมีบางส่วนไม่ชัดเจน จะแสดงผลว่า incomplete และขาดข้อมูลส่วนไหนอยู่
2. การโต้ตอบหลายรอบ (Interactive ) : หากข้อมูลไม่ครบ ระบบจะสามารถแจ้งกลับว่าขาดข้อมูลอะไร และขอข้อมูลเป็นไฟล์เสียงเพิ่ม

-----

## สถาปัตยกรรมของระบบ (Architecture)

โครงสร้างการทำงานโดยรวมเป็นดังนี้

Audio Input (.wav)  
↓  
Speech-to-Text (Faster-Whisper)  
↓  
Text Normalization  
↓  
Information Extraction (LLM)  
↓  
Rule-based Validation & Normalization  
↓  
Structured JSON Output



### องค์ประกอบหลัก

| ส่วน | หน้าที่ |
|---|---|
| STT | แปลงเสียงพูดเป็นข้อความ |
| LLM | ดึงข้อมูลเชิงโครงสร้างจากข้อความ |
| Rule-based Logic | ตรวจสอบความถูกต้องของข้อมูล |

-----

## การแบ่งส่วนการใช้งาน AI และ Rule-based

ระบบนี้ไม่ได้ใช้ AI ทั้งหมด แต่เลือกใช้ในจุดที่เหมาะสม

ส่วนที่ใช้ AI / ML คือ
- **Speech-to-Text** : Faster-Whisper (Whisper large)
- **Information Extraction** : LLM (ผ่าน Ollama)

### AI ถูกใช้ในส่วนที่:
- ข้อมูลมีความไม่แน่นอน
- ภาษามีความหลากหลาย
- ผู้ใช้พูดแบบไม่ระบุ label ชัดเจน
- รองรับทั้งภาษาไทยและภาษาอังกฤษ

### ส่วนที่ใช้ Rule-based (จุดที่ต้องการความถูกต้องแบบกำหนดชัดเจน)
- ตรวจสอบเบอร์โทรศัพท์ว่าครบ 10 หลักหรือไม่
- ตรวจสอบรูปแบบป้ายทะเบียน
- แปลงคำอ่านป้ายทะเบียน (กอไก่ → ก)
- ตัดสินว่าข้อมูลครบหรือไม่ครบ

-----

## การเลือกใช้ Speech-to-Text (STT)
เหตุผลที่เลือก Faster-Whisper:
- ทำงานแบบ local 100%
- รองรับภาษาไทยและอังกฤษได้ดี
- ไม่มีค่าใช้จ่าย
- ควบคุม model / parameter ได้เอง
- คุณภาพดีในสภาพเสียงไม่สมบูรณ์
  
การตั้งค่าที่ใช้:
- Model: `large`
- Beam size: `10`
- VAD filter: เปิดใช้งาน
  
---

## การเลือกใช้ LLM
เหตุผลที่เลือก Ollama:
- ทำงานบนเครื่อง local
- ไม่มีค่าใช้จ่าย
- ควบคุม prompt ได้เต็มที่
- ไม่มีปัญหาเรื่อง privacy

---

## การรองรับภาษาไทยและภาษาอังกฤษ

### ความสามารถของระบบ
- รองรับเสียงพูดและ transcript ภาษาไทยและภาษาอังกฤษ ( faster whisper รองรับทั้งสองภาษา )
- รองรับการตอบแบบ implicit (ไม่ต้องพูด label ชัดเจน)

ตัวอย่าง: “My gender is male, I am a male”


---

## ข้อจำกัดของไฟล์เสียง (Input Constraints)

ระบบรองรับเฉพาะไฟล์เสียง **`.wav`** ไฟล์อื่นต้องแปลงเป็น `.wav` ก่อนใช้งาน

### เหตุผล
- ลดปัญหา codec
- ควบคุม sample rate ได้ง่าย
- ลด error ใน STT
- ทำให้ pipeline เสถียร


---

## รูปแบบการรันระบบ

### 1) Pipeline Mode (Single-pass)

ไฟล์หลัก: `pipeline_full.py`

ลักษณะ:
- รับไฟล์เสียง 1 ไฟล์
- ประมวลผลครั้งเดียว
- หากข้อมูลที่ได้ไม่ครบ จะขึ้นแสดงผลว่าขาดข้อมูลส่วนไหนอยู่ แต่ไม่มีการถามกลับ
  
### 2) Interactive Mode (Multi-turn)

ไฟล์หลัก: `interactive_agent.py`

ลักษณะ:
- รันแบบ agent
- ถ้าข้อมูลไม่ครบ จะมีการถามเพิ่มเฉพาะ field ที่ขาด และให้ใส่ไฟล์เสียงเพิ่ม

---

## การจัดการ Uncertainty (ความไม่ชัดเจน)

ระบบจะถามกลับเมื่อ:
- Field ใดไม่มีข้อมูล
- ข้อมูลไม่ผ่าน validation
- STT หรือ LLM ไม่มั่นใจ

ระบบจะไม่มีการเดาเอง เลือกความถูกต้องมาก่อนความเร็ว

---

## กฎการตรวจสอบข้อมูล (Validation)

### เบอร์โทรศัพท์
- ต้องเป็นตัวเลข 10 หลัก
- ไม่มีขีดหรือช่องว่าง

### ชื่อ / นามสกุล
- ต้องไม่เป็นคำบอก label เช่น “ชื่อ”, “นามสกุล”

### ป้ายทะเบียน
- รองรับไทย / อังกฤษ
- แปลงเป็นรูปแบบย่อ (เช่น `กข1234`, `AB1234`)
- รองรับคำอ่านภาษาไทย (เช่น `กอไก่` )

---

## โครงสร้าง Repository
├── pipeline_full.py  
├── interactive_agent.py  
├── validator.py  
├── plate_normalizer.py  
├── romanize.py  
├── api_server.py  
└── README.md  

